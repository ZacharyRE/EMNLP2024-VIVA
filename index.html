
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="description"
          content="VIVA : A Benchmark for Vision-Grounded Decision-Making with Human Values">
    <meta name="keywords" content="VLM, Humor Understanding, Juxtaposition">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>VIVA : A Benchmark for Vision-Grounded Decision-Making with Human Values</title>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
          rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet"
          href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/css/bootstrap.min.css"
          integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">


    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>

</head>
<body>

<section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title"><br>VIVA : A Benchmark for Vision-Grounded Decision-Making with Human Values
            </h1>
                <div class="is-size-4 publication-authors">
                <!-- Paper authors -->
                  <span class="author-block">
                  <a href="https://derekhu.com/" target="_blank"><br>Zhe Hu*<sup>1</sup></a>,</span>
                  <span class="author-block">
                  <a href="https://zacharyre.github.io/" target="_blank">Yixiao Ren<sup>2</sup></a>,</span>
                  <span class="author-block">
                  <a href="https://www4.comp.polyu.edu.hk/~jing1li/" target="_blank">Jing Li<sup>1</sup></a>,</span>
                  </span>
                </div>

                <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>The Hong Kong Polytechnic University<br>
                    </span>
                </div>
                <div class="is-size-4 publication-authors"> <font color = 'red';><b>EMNLP 2024(main)</b></font><br> </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://2024.emnlp.org/program/accepted_main_conference/" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- ArXiv abstract Link -->
                  <span class="link-block">
                    <a href="https://arxiv.org/pdf/2407.03000" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/Derekkk/VIVA_EMNLP24" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Dataset & Code</span>
                  </a>
                </span>

            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser image-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div class="image-container">
        <!-- The introduction example image -->
        <img src="static/images/intro_example.png" alt="MY ALT TEXT" style="width: 900px;">
      </div>
      <div class="content has-text-justified">
        <p>
            We introduce the <span class="is-size-5 dnerf"><b>VIVA</b> </span>, a benchmark for vision-grounded decision-making driven by human values, which is the first to  examine their multimodal capabilities in lever-aging human values to make decisions under a vision-depicted situation.
        </p>
        
    </div>
  </div>
</section>
<!-- End teaser image -->

<!-- Abstract -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div class="columns is-centered has-text-centered">
        <h2 class="title is-3">Abstract</h2>
      </div> 
      <div class="content has-text-justified">
        <p>
            Large vision language models (VLMs) have
            demonstrated significant potential for integration into daily life, making it crucial for them to incorporate human values when making decisions in real-world situations. This paper
            introduces VIVA, a benchmark for VIsion-grounded decision-making driven by humanVAlues. While most large VLMs focus onphysical-level skills, our work is the first to examine their multimodal capabilities in leveraging human values to make decisions under
            a vision-depicted situation. VIVA contains
            1,240 images depicting diverse real-world situations and the manually annotated decisions
            grounded in them. Given an image there, the
            model should select the most appropriate action to address the situation and provide the
            relevant human values and reason underlying
            the decision. Extensive experiments based on
            VIVA show the limitation of VLMs in using
            human values to make multimodal decisions.
            Further analyses indicate the potential benefits of exploiting action consequences and predicted human values. 
        </p>
    </div>
  </div>
</section>
<!-- End Abstract -->

<!-- Dataset overview -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div class="columns is-centered has-text-centered">
        <h2 class="title is-3"><span class="dnerf">VIVA</span> Dataset Overview</h2>
      </div>
      <div class="content has-text-justified">
        <p>
          Our benchmark <span class="dnerf">VIVA</span> dataset mainly focus on the task of human-centered decision-making. There are two level in our benchmark: Level 1 is the <b>action level</b>, and the level 2 is the <b>reasoning level</b>, for each example, the following components are included:<br><br>
            (1) An image indicating a real-life scenario with the need of intervention of hunman.
            <br>
            (2) 4 options as the cadidate actions(Level1)
            <br>
            (3) Positive vlaues and negative values (level2).
            <br>
            (4) The detailed reason illstrainting the behavior (level2).
            <br>
            Based on the above component, we constructed various tasks on 9 different situations.
        </p>

    </div>
  </div>
</section>
<!-- End pipeline overview -->

<!-- Pipeline overview -->
<section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div class="columns is-centered has-text-centered">
        <h2 class="title is-3">Data Construction Pipeline</h2>
      </div>
      <div class="item">
        <img src="static/images/pipeline.jpg" alt="MY ALT TEXT"/>
      </div>
      <div class="content has-text-justified">
        <p>
          <br> The process begins with brainstorming diverse textual situationdescriptions leveraging GPT. Then, we gather images corresponding to the situations described using image searches. After that,
          human annotators collaborate with GPT to write and verify the components for each task to ensure overall data quality.
        </p>
    </div>
  </div>
</section>
<!-- End Pipeline overview -->

<!-- Tasks -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="hero-body">
        <div class="columns is-centered has-text-centered">
          <h2 class="title is-3">Assessing Multimodal Model Alignment with Human Values And the Underlying Rationale

          </h2>
        </div>
        <div class="content has-text-justified">
          <p>
            We aim to conduct a comprehensive experimental study to assess VLMs' capabilities in predicting surface actions and underlying values in vision-depicted situations.The findings will
            provide valuable insights into the development of
            socially responsible and human-centered AI, which
            will be highly beneficial to the AGI advancement.
          </p>
        </div>
        <div class="image-container">
          <img src="static/images/task_example.jpg" alt="MY ALT TEXT" style="width: 900px;"/>
          <!-- Image for task example -->
        </div>
    </div>
  </div>
</section>
<!-- End Tasks -->

<!-- Error Analysis and Future Directions -->
<section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="hero-body">
        <div class="columns is-centered has-text-centered">
          <h2 class="title is-3">Error Analysis and Future Directions for Improvement</h2>
        </div>

        <div class="content has-text-justified">
          <ul>
            <li>
              <span class="is-size-5 dnerf purple-text underline"><b>Incorrect Recognition of The Situation</b></span>: The the model fails to accurately perceive and understand the visual content in the input image.
             
              <div class="image-container">
                <img src="static/images/error_analysis1.2.png" alt="MY ALT TEXT" style="width:800px"/>
              </div>
              <p>
                => <i> This highlights the importance of future work in aligning models with relevant human values for better decision-making..</i>
              </p> 
            </li>
            <!--The first image for error analysis-->
            <li>
              <p>
              <span class="is-size-5 dnerf red-text underline"><b>Action Selection with Incorrect Values</b></span>: The model selected the correct answer based on incorrect values.
              </p>
            
              <div class="image-container">
                <img src="static/images/error_analysis2.1.png" alt="MY ALT TEXT" style="width:800px"/>
              </div>
              <p>
                => <i> This highlights the importance of future work in aligning models with relevant human values for better decision-making.</i>
              </p>
            </li>
            <!--The second image for error analysis-->
            <li>
              <p>
                <span class="is-size-5 dnerf darkblue-text underline"><b>Misprioritized response to an urgent need</b></span>:The model failed to prioritize and did not identify the objects/people most in need of help or intervention.
              </p>
              <div class="image-container">
                <img src="static/images/error_analysis3.1.png" alt="MY ALT TEXT" style="width:800px"/>
              </div>
              <p>
                => <i>This indicates that VLMs often struggle to <b> prioritize </b> actions correctly, so future work and research could focus more on enhancing VLMs' ability to prioritize events effectively.</i>
              </p>
            </li>
              <!--The third image for error analysis-->
            <li>
              <p>
              <span class="is-size-5 dnerf purple-text underline"><b>Unprofessional Assistance</b></span>: VLMs may sometimes offer assistance that lacks professionalism, potentially resulting in unintended or adverse outcomes
              </p>
              <div class="image-container">
                <img src="static/images/error_analysis2.1.png" alt="MY ALT TEXT" style="width:800px"/>
              </div>
              <p>
                => <i>. It highlights the need for future
                  efforts to incorporate better consequence prediction modules for accurate decision-making.</i>
              </p>
            </li>
             <!--The fourth image for error analysis-->
          </ul> 
        </div>
    </div>
  </div>
</section>
<!-- End Error Analysis and Future Directions -->


<!-- Potential Applications -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="hero-body">
        <div class="columns is-centered has-text-centered">
          <h2 class="title is-3">Prospective Applications</h2>
        </div>

        <div class="content has-text-justified">
          <ul>
            <li>
              <p>
                <span class="is-size-5 dnerf"><b>Model Evaluation and Assessment</b></span></br>
              We collected data and constructed a benchmark based on nine categories of real-world scenarios, making this dataset comprehensive and capable of evaluating model alignment with human values across multiple aspects and levels.
              </p>
            
              <div class="image-container">
                <img src="static/images/category_of_situation.png" alt="FAIL TO LOAD" style="width: 700px;"/>
              </div>
              </br>
            </li>
                <!-- The first image-model evaluation -->
            <li>
              <p>
              This outlines our approach to processing and analyzing the model's performance on this benchmark, as well as its ability to understand and align with human values.
              </p>

              <div class="image-container">
                <img src="static/images/analysis_result.png" alt="FAIL TO LOAD" style="width: 900px;" />
              </div>
              </br>
            </li>

            <li>
              <p>
                <span class="is-size-5 dnerf"><b>VLM image understanding</b></span></br>
                We analyzed and evaluated the performance of various models across nine different real-world scenarios, identifying weaknesses and areas for future improvement. This analysis can enhance the models' alignment with human values, making them more effective and safe in <b>human-interactive domains</b>(e.g., home robotics, virtual assistants, etc.)Additionally, it provides valuable insights for future model development directions.
              </p>
              <div class="image-container">
                <img src="static/images/type_result.png" alt="FAIL TO LOAD" style="width: 800px;"/>
              </div>
            </li>

          </ul> 
        </div>
    </div>
  </div>
</section>
<!-- End Error Analysis and Future Directions -->


<!-- Potential Applications -->
<section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="hero-body">
        <div class="columns is-centered has-text-centered">
          <h2 class="title is-3">Ethics Statement</h2>
        </div>

        <div class="content has-text-justified">
          <ul>
            <li>
              <p>
                <span class="is-size-5 dnerf"><b>Copyright and License</b></span></br>
                Copyright and License. All images in VIVA
                benchmark are sourced from publicly available
                content on social media platforms. We guarantee
                compliance with copyright regulations by utilizing original links to each image without infringement. Additionally, we commit to openly sharing
                our annotated benchmark, with providing the corresponding link to each image. Throughout the
                image collection process, we meticulously review
                samples, filtering out any potentially offensive or
                harmful content.
              </p>
            </li>

            <li>
              <p>
                <span class="is-size-5 dnerf"><b>Data Annotations with GPT</b></span></br>
                Data Annotations with GPT. Our data annotation
                involves leveraging GPT to produce initial versions
                of each component, which are then verified and
                revised by human annotators. Despite our best efforts to ensure the quality of the annotations, we
                acknowledge that utilizing large language models
                may introduce potential bias. The generated results
                may tend to favor certain majority groups. Furthermore, our annotation and task design prioritize
                collective norms and values. For instance, when
                presented with a scenario involving a visually impaired individual struggling to cross the road, our
                action selection favors providing assistance rather
                than ignoring the situation and taking no action.
                To mitigate bias, our annotation process includes
                rigorous quality checks, with each sample annotated and reviewed by different human annotators
                to reduce ambiguity
              </p>


            </li>

            <li>
              <p>
                <span class="is-size-5 dnerf"><b>Data Annotation and Potential Bias</b></span></br>
                Six annotators are engaged in our annotation process. All
                annotators are proficient English speakers and are
                based in English speaking areas. Before the annotation, we conducted thorough training and task
                briefing for our annotators, as well as a trial annotation to ensure they have a clear understanding of
                the research background and the use of the data.
                We compensate these annotators with an average
                hourly wage of $10, ensuring fair remuneration for
                their contributions. The data collection process is
                conducted under the guidance of the organization
                ethics review system to ensure the positive societal
                impact of the project.
              </p>
            </li>

          </ul> 
        </div>
    </div>
  </div>
</section>
<!-- End Error Analysis and Future Directions -->


<!-- Citation -->
<section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
        <h2 class="title">Citation</h2>
        <p>If you find our work helpful, please consider cite us:</p>
        <pre><code>@article{hu2024cracking,
          title={VIVA : A Benchmark for Vision-Grounded Decision-Making with Human Values},
          author={Zhe Hu, Yixiao Ren, Jing Li},
          journal={arXiv preprint arXiv:2407.03000},
          year={2024}
          }
</code></pre>
    </div>
</section>
<!-- End pipeline overview -->


<footer class="footer">
    <div class="container">
        <div class="columns is-centered">
            <div class="column is-10">
                <div class="content">
                    <p>The comics on the website are created by Artist <a href="https://x.com/like_gudim">Gudim</a>.</p>

                    <p>Website template borrowed from <a href="https://github.com/vulab-AI/View-consistent_Object_Removal_in_Radiance_Fields">View-consistent Object Removal in Radiance Fields</a>.</p>
                    <p>
                        This website is licensed under a <a rel="license"
                                                            href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                        Commons Attribution-ShareAlike 4.0 International License</a>.
                    </p>
                    <!-- <p>
                        This means you are free to borrow the <a
                            href="https://github.com/ornerf/ornerf.github.io">source code</a> of this website,
                        we just ask that you link back to this page in the footer.
                        Please remember to remove the analytics code included in the header of the website which
                        you do not want on your website.
                    </p> -->
                </div>
            </div>
        </div>
    </div>
</footer>

</body>
</html>
